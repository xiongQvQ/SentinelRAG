#!/usr/bin/env python3
"""
é¢„ä¸‹è½½ç»´åŸºç™¾ç§‘æ•°æ®è„šæœ¬
Pre-download Wikipedia data for RAG system
è¿è¡Œæ­¤è„šæœ¬é¢„å…ˆä¸‹è½½æ‰€æœ‰éœ€è¦çš„æ•°æ®ï¼Œé¿å…åœ¨å¯åŠ¨åº”ç”¨æ—¶ä¸‹è½½
"""

import os
import sys
import json
import logging
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.data_collector import WikipediaDataCollector

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('predownload.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DataPreloader:
    """æ•°æ®é¢„åŠ è½½å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.collector = WikipediaDataCollector(data_dir)
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(data_dir, exist_ok=True)
        
        # é¢„å®šä¹‰çš„è¯é¢˜å’Œæ–‡ç« æ•°é‡é…ç½®
        self.topic_config = {
            # æ ¸å¿ƒAIè¯é¢˜ - æ›´å¤šæ–‡ç« 
            "Artificial Intelligence": 5,
            "Machine Learning": 5,
            "Deep Learning": 4,
            "Neural Networks": 3,
            
            # ä¸“é—¨é¢†åŸŸ
            "Natural Language Processing": 4,
            "Computer Vision": 4,
            "Robotics": 3,
            "Data Science": 3,
            
            # ç›¸å…³æŠ€æœ¯
            "Python Programming": 3,
            "Data Mining": 2,
            "Big Data": 2,
            "Cloud Computing": 2,
            
            # åº”ç”¨é¢†åŸŸ
            "Autonomous Vehicles": 2,
            "Healthcare AI": 2,
            "Fintech": 2,
            "Cybersecurity": 2
        }
    
    def download_all_data(self, skip_existing: bool = True) -> bool:
        """ä¸‹è½½æ‰€æœ‰æ•°æ®"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
            processed_file = os.path.join(self.data_dir, "processed_wikipedia_articles.json")
            if skip_existing and os.path.exists(processed_file):
                with open(processed_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                logger.info(f"å‘ç°å·²æœ‰æ•°æ®: {len(existing_data)} ä¸ªæ–‡æ¡£å—")
                
                response = input("å‘ç°å·²æœ‰æ•°æ®ï¼Œæ˜¯å¦è·³è¿‡ä¸‹è½½ï¼Ÿ(y/n): ").lower()
                if response == 'y':
                    return True
            
            logger.info("å¼€å§‹ä¸‹è½½ç»´åŸºç™¾ç§‘æ•°æ®...")
            logger.info(f"å°†ä¸‹è½½ {len(self.topic_config)} ä¸ªè¯é¢˜çš„æ–‡ç« ")
            
            # æ˜¾ç¤ºå°†è¦ä¸‹è½½çš„å†…å®¹
            total_articles = sum(self.topic_config.values())
            logger.info(f"é¢„è®¡ä¸‹è½½çº¦ {total_articles} ç¯‡æ–‡ç« ")
            
            # åˆ†æ‰¹æ¬¡æ”¶é›†æ•°æ®
            all_articles = []
            
            for i, (topic, count) in enumerate(self.topic_config.items(), 1):
                logger.info(f"æ­£åœ¨å¤„ç†è¯é¢˜ {i}/{len(self.topic_config)}: {topic} (ç›®æ ‡: {count}ç¯‡)")
                
                try:
                    # æœç´¢å¹¶è·å–æ–‡ç« 
                    article_titles = self.collector.search_articles(topic, count + 2)  # å¤šæœç´¢å‡ ä¸ªä»¥é˜²å¤±è´¥
                    
                    topic_articles = []
                    for title in article_titles[:count]:  # åªå–æŒ‡å®šæ•°é‡
                        article = self.collector.get_article_content(title)
                        if article:
                            article["topic"] = topic
                            topic_articles.append(article)
                            logger.info(f"  âœ“ è·å–æ–‡ç« : {title}")
                        else:
                            logger.warning(f"  âœ— æ— æ³•è·å–æ–‡ç« : {title}")
                    
                    all_articles.extend(topic_articles)
                    logger.info(f"è¯é¢˜ '{topic}' å®Œæˆ: è·å–äº† {len(topic_articles)} ç¯‡æ–‡ç« ")
                    
                    # ä¿å­˜ä¸­é—´è¿›åº¦
                    if len(all_articles) % 10 == 0:
                        self._save_progress(all_articles, "temp_articles.json")
                    
                except Exception as e:
                    logger.error(f"å¤„ç†è¯é¢˜ '{topic}' æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.info(f"æ‰€æœ‰æ–‡ç« æ”¶é›†å®Œæˆï¼æ€»å…±è·å–äº† {len(all_articles)} ç¯‡æ–‡ç« ")
            
            # é¢„å¤„ç†å†…å®¹
            logger.info("æ­£åœ¨é¢„å¤„ç†æ–‡ç« å†…å®¹...")
            processed_articles = self.collector.preprocess_content(all_articles)
            logger.info(f"å¤„ç†å®Œæˆï¼ç”Ÿæˆäº† {len(processed_articles)} ä¸ªæ–‡æ¡£å—")
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            final_file = self.collector.save_articles(processed_articles, "processed_wikipedia_articles.json")
            
            # ç”Ÿæˆæ•°æ®ç»Ÿè®¡
            self._generate_data_stats(processed_articles)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_file = os.path.join(self.data_dir, "temp_articles.json")
            if os.path.exists(temp_file):
                os.remove(temp_file)
            
            logger.info("ğŸ‰ æ•°æ®é¢„ä¸‹è½½å®Œæˆï¼")
            logger.info(f"æ•°æ®æ–‡ä»¶: {final_file}")
            logger.info("ç°åœ¨å¯ä»¥å¯åŠ¨åº”ç”¨è€Œæ— éœ€ç­‰å¾…æ•°æ®ä¸‹è½½")
            
            return True
            
        except Exception as e:
            logger.error(f"é¢„ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return False
    
    def _save_progress(self, articles: List[Dict[str, Any]], filename: str):
        """ä¿å­˜ä¸­é—´è¿›åº¦"""
        filepath = os.path.join(self.data_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        logger.info(f"è¿›åº¦å·²ä¿å­˜: {len(articles)} ç¯‡æ–‡ç« ")
    
    def _generate_data_stats(self, processed_articles: List[Dict[str, Any]]):
        """ç”Ÿæˆæ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_chunks": len(processed_articles),
            "unique_articles": len(set(article["title"] for article in processed_articles)),
            "topics": {},
            "generated_at": datetime.now().isoformat()
        }
        
        # ç»Ÿè®¡æ¯ä¸ªè¯é¢˜çš„æ–‡ç« æ•°
        for article in processed_articles:
            topic = article.get("topic", "unknown")
            if topic not in stats["topics"]:
                stats["topics"][topic] = {"chunks": 0, "articles": set()}
            stats["topics"][topic]["chunks"] += 1
            stats["topics"][topic]["articles"].add(article["title"])
        
        # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        for topic in stats["topics"]:
            stats["topics"][topic]["unique_articles"] = len(stats["topics"][topic]["articles"])
            stats["topics"][topic]["articles"] = list(stats["topics"][topic]["articles"])
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = os.path.join(self.data_dir, "data_stats.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info("=== æ•°æ®ç»Ÿè®¡ ===")
        logger.info(f"æ–‡æ¡£å—æ€»æ•°: {stats['total_chunks']}")
        logger.info(f"ç‹¬ç‰¹æ–‡ç« æ•°: {stats['unique_articles']}")
        logger.info("å„è¯é¢˜åˆ†å¸ƒ:")
        for topic, info in stats["topics"].items():
            logger.info(f"  {topic}: {info['unique_articles']} ç¯‡æ–‡ç« , {info['chunks']} ä¸ªå—")
    
    def check_data_status(self):
        """æ£€æŸ¥å½“å‰æ•°æ®çŠ¶æ€"""
        logger.info("=== æ•°æ®çŠ¶æ€æ£€æŸ¥ ===")
        
        # æ£€æŸ¥åŸå§‹æ•°æ®
        processed_file = os.path.join(self.data_dir, "processed_wikipedia_articles.json")
        if os.path.exists(processed_file):
            with open(processed_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info(f"âœ… å·²æœ‰å¤„ç†åçš„æ•°æ®: {len(data)} ä¸ªæ–‡æ¡£å—")
        else:
            logger.info("âŒ æ²¡æœ‰æ‰¾åˆ°å¤„ç†åçš„æ•°æ®")
        
        # æ£€æŸ¥ç»Ÿè®¡æ–‡ä»¶
        stats_file = os.path.join(self.data_dir, "data_stats.json")
        if os.path.exists(stats_file):
            with open(stats_file, 'r', encoding='utf-8') as f:
                stats = json.load(f)
            logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡å¯ç”¨ (ç”Ÿæˆäº: {stats['generated_at']})")
        else:
            logger.info("ğŸ“Š æ²¡æœ‰æ•°æ®ç»Ÿè®¡æ–‡ä»¶")
        
        # æ£€æŸ¥å‘é‡å­˜å‚¨
        vector_store_dir = "vector_store"
        if os.path.exists(os.path.join(vector_store_dir, "faiss_index.index")):
            logger.info("âœ… å‘é‡å­˜å‚¨å·²å­˜åœ¨")
        else:
            logger.info("âš ï¸ å‘é‡å­˜å‚¨å°šæœªåˆ›å»º")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç»´åŸºç™¾ç§‘æ•°æ®é¢„ä¸‹è½½å·¥å…·")
    print("=" * 50)
    
    preloader = DataPreloader()
    
    # æ˜¾ç¤ºèœå•
    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("1. æ£€æŸ¥æ•°æ®çŠ¶æ€")
        print("2. ä¸‹è½½æ‰€æœ‰æ•°æ®")
        print("3. å¼ºåˆ¶é‡æ–°ä¸‹è½½")
        print("4. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
        
        if choice == "1":
            preloader.check_data_status()
            
        elif choice == "2":
            success = preloader.download_all_data(skip_existing=True)
            if success:
                print("\nâœ… æ•°æ®ä¸‹è½½å®Œæˆï¼ç°åœ¨å¯ä»¥è¿è¡Œ: streamlit run app_gemini.py")
            else:
                print("\nâŒ æ•°æ®ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé”™è¯¯æ—¥å¿—")
                
        elif choice == "3":
            print("âš ï¸ è¿™å°†åˆ é™¤ç°æœ‰æ•°æ®å¹¶é‡æ–°ä¸‹è½½")
            confirm = input("ç¡®è®¤ç»§ç»­ï¼Ÿ(y/n): ").lower()
            if confirm == 'y':
                success = preloader.download_all_data(skip_existing=False)
                if success:
                    print("\nâœ… æ•°æ®é‡æ–°ä¸‹è½½å®Œæˆï¼")
                else:
                    print("\nâŒ æ•°æ®ä¸‹è½½å¤±è´¥")
                    
        elif choice == "4":
            print("ğŸ‘‹ å†è§ï¼")
            break
            
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-4")

if __name__ == "__main__":
    main()